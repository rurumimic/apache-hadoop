# Single Node Setup

## PATH

Set `HADOOP_HOME`

```bash
echo 'export HADOOP_HOME=/home/$USER/hadoop' >> ~/.bash_profile
echo 'export PATH=$PATH:$HADOOP_HOME/bin' >> ~/.bash_profile
echo 'export PATH=$PATH:$HADOOP_HOME/sbin' >> ~/.bash_profile
source ~/.bash_profile
```

## Configuration

### hadoop-env.sh

```bash
vi  $HADOOP_HOME/etc/hadoop/hadoop-env.sh
```

```bash
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk/jre
```

### core-site.xml

- Host: `192.168.xxx.xxx`
- Port: `9000`

```bash
vi $HADOOP_HOME/etc/hadoop/core-site.xml
```

```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://192.168.xxx.xxx:9000</value>
    </property>
</configuration>
```

### hdfs-site.xml

```bash
vi $HADOOP_HOME/etc/hadoop/hdfs-site.xml
```

```xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>
```

### mapred-site.xml

```bash
vi $HADOOP_HOME/etc/hadoop/mapred-site.xml
```

```xml
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.application.classpath</name>
        <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>
    </property>
</configuration>
```

### yarn-site.xml

```bash
vi $HADOOP_HOME/etc/hadoop/yarn-site.xml
```

```xml
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
</configuration>
```

## Execution

### Format File System

```bash
hdfs namenode -format
```

### Start Daemons

```bash
start-dfs.sh
start-yarn.sh
```

### Stop Daemons

```bash
stop-yarn.sh
stop-dfs.sh
```

### Make Working Directory

```bash
hdfs dfs -mkdir -p /user/$USER
```

### Test

#### Load test files

```bash
hdfs dfs -mkdir input
hdfs dfs -put $HADOOP_HOME/etc/hadoop/*.xml input
```

View the input files

```bash
hdfs dfs -ls -R /
```

#### Run a MapReduce job

Check your hadoop version

```bash
hadoop jar \
$HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.y.z.jar \
grep input output 'dfs[a-z.]+'
```

#### View output files

```bash
hdfs dfs -cat output/*
```

```bash
1       dfsadmin
1       dfs.replication
```